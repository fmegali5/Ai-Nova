import { useState, useEffect, useRef } from "react";
import { Sparkles } from "lucide-react";
import { axiosInstance } from "../lib/axios";
import toast from "react-hot-toast";

function VoiceAssistant({ darkMode, selectedModel }) {
  const [isActive, setIsActive] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const recognitionRef = useRef(null);
  const synthRef = useRef(window.speechSynthesis);
  const isProcessingRef = useRef(false);
  const conversationHistoryRef = useRef([]);
  const isActiveRef = useRef(false);

  const isChatSpeechActive = () => {
    const chatSpeechButton = document.querySelector('[data-recording="true"]');
    return !!chatSpeechButton;
  };

  useEffect(() => {
    isActiveRef.current = isActive;
  }, [isActive]);

  useEffect(() => {
    if (!isActive) {
      if (recognitionRef.current) {
        try {
          recognitionRef.current.abort();
          recognitionRef.current = null;
        } catch (e) {
          console.log("Recognition already stopped");
        }
      }
      return;
    }

    if (isChatSpeechActive()) {
      console.log("âš ï¸ Chat speech is active");
      toast.error("ÙŠØ±Ø¬Ù‰ Ø¥ÙŠÙ‚Ø§Ù ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ Ø£ÙˆÙ„Ø§Ù‹");
      setIsActive(false);
      return;
    }

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    
    if (!SpeechRecognition) {
      console.warn("Speech recognition not supported");
      toast.error("Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª ØºÙŠØ± Ù…ØªØ§Ø­. Ø§Ø³ØªØ®Ø¯Ù… Chrome Ø£Ùˆ Edge.");
      return;
    }

    const recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = false;
    recognition.lang = 'ar-SA'; // âœ… Arabic recognition
    recognition.maxAlternatives = 1;

    recognition.onstart = () => {
      console.log("ğŸ¤ Listening started...");
      setIsListening(true);
    };

    recognition.onresult = (event) => {
      for (let i = event.resultIndex; i < event.results.length; i++) {
        if (event.results[i].isFinal) {
          const transcript = event.results[i][0].transcript.trim();
          
          console.log("ğŸ¤ Heard:", transcript);
          
          if (transcript && transcript.length > 0) {
            console.log("âœ… Command detected:", transcript);
            handleVoiceCommand(transcript);
          }
        }
      }
    };

    recognition.onspeechstart = () => {
      console.log("ğŸ—£ï¸ Speech detected");
    };

    recognition.onspeechend = () => {
      console.log("ğŸ¤ Speech ended - waiting for next command...");
    };

    recognition.onerror = (event) => {
      if (event.error !== 'no-speech' && event.error !== 'aborted') {
        console.error("Recognition error:", event.error);
      }
      
      if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
        toast.error("ØªÙ… Ø±ÙØ¶ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„.");
      }
    };

    recognition.onend = () => {
      console.log("ğŸ¤ Recognition ended");
      setIsListening(false);
      
      if (isActiveRef.current && recognitionRef.current) {
        try {
          setTimeout(() => {
            if (isActiveRef.current && recognitionRef.current) {
              recognitionRef.current.start();
              console.log("ğŸ¤ Listening restarted...");
            }
          }, 300);
        } catch (error) {
          console.log("Error restarting recognition:", error);
        }
      } else {
        console.log("âŒ Not restarting - isActive is false");
      }
    };

    recognitionRef.current = recognition;

    try {
      recognition.start();
      console.log("âœ… Voice recognition started");
    } catch (error) {
      console.error("Error starting voice recognition:", error);
      toast.error("ÙØ´Ù„ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø§ÙŠÙƒ");
    }

    return () => {
      if (recognitionRef.current) {
        try {
          recognitionRef.current.abort();
          recognitionRef.current = null;
        } catch (e) {
          console.log("Recognition already stopped");
        }
      }
    };
  }, [isActive]);

  const handleVoiceCommand = async (userMessage) => {
    if (isProcessingRef.current) {
      console.log("âš ï¸ Already processing");
      return;
    }

    isProcessingRef.current = true;
    console.log("ğŸ“¤ Sending:", userMessage);

    try {
      const newHistory = [
        ...conversationHistoryRef.current,
        { role: 'user', content: userMessage }
      ];
      conversationHistoryRef.current = newHistory;

      const res = await axiosInstance.post("/messages/voice", {
        message: userMessage,
        model: selectedModel,
        conversationHistory: newHistory.slice(-10)
      });

      const aiResponse = res.data.message;

      conversationHistoryRef.current = [
        ...newHistory,
        { role: 'assistant', content: aiResponse }
      ];

      console.log("ğŸ“¥ Response:", aiResponse);
      speakText(aiResponse);

    } catch (error) {
      console.error("âŒ Error:", error);
      toast.error("Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„");
      speakText("Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© ØªØ§Ù†ÙŠØ©.");
    } finally {
      isProcessingRef.current = false;
    }
  };

  const speakText = (text) => {
    if (!synthRef.current) {
      console.error("âŒ Speech synthesis not available");
      return;
    }

    console.log("ğŸ¤ About to speak:", text);

    synthRef.current.cancel();

    const utterance = new SpeechSynthesisUtterance(text);
    
    // âœ… Try multiple Arabic dialects
    const voices = synthRef.current.getVoices();
    
    console.log("ğŸ™ï¸ Total voices available:", voices.length);
    
    // âœ… Print all Arabic voices for debugging
    const arabicVoices = voices.filter(v => v.lang.startsWith('ar'));
    console.log("ğŸ‡¸ğŸ‡¦ Arabic voices found:", arabicVoices.length);
    arabicVoices.forEach((v, i) => {
      console.log(`  ${i + 1}. ${v.name} (${v.lang}) - Local: ${v.localService}`);
    });
    
    // âœ… Priority order: ar-EG > ar-SA > ar-AE > ar > any Arabic
    const egyptianVoice = voices.find(v => v.lang === 'ar-EG');
    const saudiVoice = voices.find(v => v.lang === 'ar-SA');
    const uaeVoice = voices.find(v => v.lang === 'ar-AE');
    const genericArabic = voices.find(v => v.lang === 'ar');
    const anyArabicVoice = voices.find(v => v.lang.startsWith('ar'));
    
    const selectedVoice = egyptianVoice || saudiVoice || uaeVoice || genericArabic || anyArabicVoice;
    
    if (selectedVoice) {
      utterance.voice = selectedVoice;
      utterance.lang = selectedVoice.lang;
      console.log("âœ… SELECTED:", selectedVoice.name, `(${selectedVoice.lang})`);
    } else {
      utterance.lang = 'ar-SA'; // Fallback
      console.log("âš ï¸ No Arabic voice found! Using lang: ar-SA");
    }
    
    utterance.rate = 1.0;
    utterance.pitch = 1.0;
    utterance.volume = 1.0;

    utterance.onstart = () => {
      console.log("ğŸ¤ Speaking with:", utterance.voice?.name || 'default', `(${utterance.lang})`);
    };

    utterance.onend = () => {
      console.log("âœ… Speech ended");
    };

    utterance.onerror = (event) => {
      console.error("âŒ Speech error:", event.error);
    };

    try {
      synthRef.current.speak(utterance);
    } catch (error) {
      console.error("âŒ Error speaking:", error);
    }
  };

  const toggleVoiceAssistant = () => {
    if (isChatSpeechActive()) {
      toast.error("ÙŠØ±Ø¬Ù‰ Ø¥ÙŠÙ‚Ø§Ù ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ Ø£ÙˆÙ„Ø§Ù‹");
      return;
    }

    if (isActive) {
      console.log("ğŸ›‘ Stopping Voice Assistant...");
      
      setIsActive(false);
      isActiveRef.current = false;
      
      if (recognitionRef.current) {
        try {
          recognitionRef.current.abort();
          recognitionRef.current = null;
        } catch (e) {
          console.log("Error aborting recognition:", e);
        }
      }
      
      if (synthRef.current) {
        synthRef.current.cancel();
      }
      
      setIsListening(false);
      toast("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠ");
      console.log("âŒ Voice Assistant Stopped");
    } else {
      console.log("â–¶ï¸ Starting Voice Assistant...");
      
      setIsActive(true);
      isActiveRef.current = true;
      
      toast.success("ğŸ¤ Ø§Ù„Ù…Ø§ÙŠÙƒ Ù…ÙØªÙˆØ­ - ØªÙƒÙ„Ù…!");
      console.log("âœ… Voice Assistant Started");
      
      speakText("Ù…Ø¹Ø§Ùƒ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø°ÙƒÙŠ Ù†ÙˆÙØ§. Ø§Ø²ÙŠÙƒ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŸ");
    }
  };

  return (
    <div 
      className="relative inline-block"
      style={{ position: 'relative' }}
    >
      <button
        onClick={toggleVoiceAssistant}
        className="rounded-full flex items-center justify-center flex-shrink-0 transition-all hover:opacity-80"
        style={{
          width: '32px',
          height: '32px',
          background: 'linear-gradient(135deg, rgb(51, 5, 130), rgb(98, 41, 255))',
          border: 'none',
          cursor: 'pointer',
          padding: 0,
          position: 'relative'
        }}
        title={isActive ? "Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠ" : "ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠ"}
      >
        <Sparkles size={16} className="text-white" />
      </button>

      {isActive && (
        <div 
          style={{
            position: 'absolute',
            bottom: '-9.5px',
            left: '50%',
            transform: 'translateX(-50%)',
            width: '8px',
            height: '8px',
            backgroundColor: '#22c55e',
            borderRadius: '50%',
            boxShadow: '0 0 10px rgba(34, 197, 94, 0.8)',
            zIndex: 10
          }}
          className="animate-pulse"
        />
      )}
    </div>
  );
}

export default VoiceAssistant;
